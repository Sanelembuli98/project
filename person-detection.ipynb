{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af3733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (8, 4096)\n",
      "Labels shape: (8, 5)\n",
      "Loaded image files:\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\00a1afdf3de82276cc2765a6_png_jpg.rf.26c0b795902b4229b8a748a4989e2d3e.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\00a50088195f1dcee818af7f_png_jpg.rf.95d1e53a6da68d5bf4e896832369ac26.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\00ad63b6f93990585417f9c3_png_jpg.rf.88b1ff59a17695f49b2e26b4ca75b59b.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\00f3149698f5d90d5198a7b7_png_jpg.rf.3f7a1642a3fc98cb975cae3d3217eaa5.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\00f81162c43a980800ecedf8_png_jpg.rf.12a2d76673da91a3c7c8c975756d6aa9.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\0a1c3327781bafc70dbb1cbc_png_jpg.rf.a2372a0fdc676c4cd455e02845faf161.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\0a3a22b9b53f81f08b289072_png_jpg.rf.c200b453b73b254f21d72f3854fdc54f.jpg\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\images\\train\\0a3eba013cdb06555cd65ba5_png_jpg.rf.d17405ed6caa29652412e865c45236e5.jpg\n",
      "Loaded label files:\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\00a1afdf3de82276cc2765a6_png_jpg.rf.26c0b795902b4229b8a748a4989e2d3e.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\00a50088195f1dcee818af7f_png_jpg.rf.95d1e53a6da68d5bf4e896832369ac26.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\00ad63b6f93990585417f9c3_png_jpg.rf.88b1ff59a17695f49b2e26b4ca75b59b.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\00f3149698f5d90d5198a7b7_png_jpg.rf.3f7a1642a3fc98cb975cae3d3217eaa5.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\00f81162c43a980800ecedf8_png_jpg.rf.12a2d76673da91a3c7c8c975756d6aa9.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\0a1c3327781bafc70dbb1cbc_png_jpg.rf.a2372a0fdc676c4cd455e02845faf161.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\0a3a22b9b53f81f08b289072_png_jpg.rf.c200b453b73b254f21d72f3854fdc54f.txt\n",
      "C:\\Users\\SaneleMbedu\\Downloads\\trainingdata\\datasets1\\labels\\train\\0a3eba013cdb06555cd65ba5_png_jpg.rf.d17405ed6caa29652412e865c45236e5.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set the directory paths\n",
    "dataset_dir = 'C:\\\\Users\\\\SaneleMbedu\\\\Downloads\\\\trainingdata\\\\datasets1'\n",
    "images_dir = os.path.join(dataset_dir, 'images')\n",
    "labels_dir = os.path.join(dataset_dir, 'labels')\n",
    "\n",
    "# Define the number of classes and class names\n",
    "num_classes = 1\n",
    "class_names = ['box']\n",
    "\n",
    "# Initialize lists to store the loaded images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load images\n",
    "image_files = glob.glob(os.path.join(images_dir, '*', '*.jpg'))\n",
    "for img_file in image_files:\n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, (64, 64))  # Resize the image as needed\n",
    "\n",
    "    # Preprocessing steps\n",
    "    # Convert to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply histogram equalization\n",
    "    equalized_img = cv2.equalizeHist(gray_img)\n",
    "    \n",
    "    # Apply Gaussian blur\n",
    "    blurred_img = cv2.GaussianBlur(equalized_img, (5, 5), 0)\n",
    "    \n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    normalized_img = blurred_img / 255.0\n",
    "    \n",
    "    images.append(normalized_img)\n",
    "\n",
    "# Load labels\n",
    "label_files = glob.glob(os.path.join(labels_dir, '*', '*.txt'))\n",
    "for lbl_file in label_files:\n",
    "    with open(lbl_file, 'r') as file:\n",
    "        # Read the contents of the label file\n",
    "        # Process the contents as needed based on the YOLO format\n",
    "        # Append the processed labels to the 'labels' list\n",
    "        # Example: Assuming the labels are in the format: class_id x_center y_center width height\n",
    "        labels_data = file.readlines()\n",
    "        processed_labels = []\n",
    "        for label in labels_data:\n",
    "            label = label.strip().split()\n",
    "            class_id = int(label[0])  # Convert class ID to integer\n",
    "            x_center = float(label[1])\n",
    "            y_center = float(label[2])\n",
    "            width = float(label[3])\n",
    "            height = float(label[4])\n",
    "            processed_labels.append([class_id, x_center, y_center, width, height])\n",
    "        if len(processed_labels) > 0:  # Check if any labels were processed\n",
    "            labels.append(processed_labels)\n",
    "\n",
    "# Ensure that the number of images and labels is the same\n",
    "if len(images) != len(labels):\n",
    "    raise ValueError(\"Number of images and labels is not consistent.\")\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform additional preprocessing on the training set\n",
    "# Reshape the images to match the input shape expected by the model\n",
    "images = images.reshape(images.shape[0], 64, 64, 1)\n",
    "\n",
    "# Reshape the images to be 2-dimensional\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Normalize the training images using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "images = scaler.fit_transform(images)\n",
    "\n",
    "# Print the shapes of the loaded images and labels\n",
    "labels = np.squeeze(labels)\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Print the file paths of the loaded images and labels\n",
    "print(\"Loaded image files:\")\n",
    "for img_file in image_files:\n",
    "    print(img_file)\n",
    "    \n",
    "print(\"Loaded label files:\")\n",
    "for lbl_file in label_files:\n",
    "    print(lbl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3690b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (32, 32, 32, 1)\n",
      "Labels shape: (8, 5)\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.8297 - accuracy: 0.0000e+00 - val_loss: 4.2427 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.5995 - accuracy: 0.6667 - val_loss: 4.2903 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.5948 - accuracy: 0.5000 - val_loss: 4.3043 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.8867 - accuracy: 0.3333 - val_loss: 4.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2007 - accuracy: 0.1667 - val_loss: 4.3365 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0594 - accuracy: 0.0000e+00 - val_loss: 4.3476 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0804 - accuracy: 0.3333 - val_loss: 4.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.4111 - accuracy: 0.3333 - val_loss: 4.3022 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.7218 - accuracy: 0.1667 - val_loss: 4.3009 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.5240 - accuracy: 0.5000 - val_loss: 4.2721 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.9772 - accuracy: 0.3333 - val_loss: 4.2690 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5142 - accuracy: 0.1667 - val_loss: 4.2661 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.2258 - accuracy: 0.5000 - val_loss: 4.2667 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.4217 - accuracy: 0.5000 - val_loss: 4.2680 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0768 - accuracy: 0.1667 - val_loss: 4.2570 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9300 - accuracy: 0.3333 - val_loss: 4.2460 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5129 - accuracy: 0.1667 - val_loss: 4.2418 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6013 - accuracy: 0.3333 - val_loss: 4.2406 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2825 - accuracy: 0.3333 - val_loss: 4.2384 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0491 - accuracy: 0.1667 - val_loss: 4.2430 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Reshape the images array to have the correct shape\n",
    "images = np.reshape(images, (-1, 32, 32, 1))\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Ensure the number of samples in images and labels match\n",
    "num_samples = min(images.shape[0], labels.shape[0])\n",
    "images = images[:num_samples]\n",
    "labels = labels[:num_samples]\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 1)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(images, labels, epochs=20, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207d278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "[0.0, 0.6666666865348816, 0.5, 0.3333333432674408, 0.1666666716337204, 0.0, 0.3333333432674408, 0.3333333432674408, 0.1666666716337204, 0.5, 0.3333333432674408, 0.1666666716337204, 0.5, 0.5, 0.1666666716337204, 0.3333333432674408, 0.1666666716337204, 0.3333333432674408, 0.3333333432674408, 0.1666666716337204]\n",
      "\n",
      "Validation Accuracy:\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Training Loss:\n",
      "[3.8297433853149414, 3.5995099544525146, 3.5948173999786377, 3.8866512775421143, 3.200716733932495, 3.05938720703125, 3.08038592338562, 4.4111199378967285, 3.721825361251831, 3.5240256786346436, 3.9772021770477295, 3.514152765274048, 3.2258098125457764, 3.4217288494110107, 3.0767860412597656, 2.930025815963745, 2.5128846168518066, 2.6013026237487793, 3.282503843307495, 3.049082040786743]\n",
      "\n",
      "Validation Loss:\n",
      "[4.24269962310791, 4.290294647216797, 4.304269313812256, 4.362902641296387, 4.336537837982178, 4.347631454467773, 4.333032608032227, 4.302191257476807, 4.3009185791015625, 4.272123336791992, 4.2689690589904785, 4.266114711761475, 4.266664505004883, 4.268019676208496, 4.256997108459473, 4.246010780334473, 4.241796970367432, 4.240633487701416, 4.238367557525635, 4.242997169494629]\n"
     ]
    }
   ],
   "source": [
    "# Print the training and validation accuracy for each epoch\n",
    "print(\"Training Accuracy:\")\n",
    "print(history.history['accuracy'])\n",
    "print(\"\\nValidation Accuracy:\")\n",
    "print(history.history['val_accuracy'])\n",
    "\n",
    "# Print the training and validation loss for each epoch\n",
    "print(\"\\nTraining Loss:\")\n",
    "print(history.history['loss'])\n",
    "print(\"\\nValidation Loss:\")\n",
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69989329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "one_hot_labels = to_categorical(labels)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(one_hot_labels, axis=1), predicted_labels)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['person']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "plt.yticks(tick_marks, class_labels)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aada670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Image: a (1).jpg\n",
      "Predicted Label: 2\n",
      "\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Image: a (2).jpg\n",
      "Predicted Label: 2\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Image: a (2).jpg\n",
      "Predicted Label: 2\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Image: a (4).jpg\n",
      "Predicted Label: 2\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Image: a (5).jpg\n",
      "Predicted Label: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the new test images\n",
    "test_images = [\n",
    "    'a (1).jpg',\n",
    "    'a (2).jpg',\n",
    "    'a (2).jpg',\n",
    "    'a (4).jpg',\n",
    "    'a (5).jpg'\n",
    "]\n",
    "\n",
    "# Preprocess and predict the labels for the test images\n",
    "for image_path in test_images:\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    image = np.reshape(image, (1, 32, 32, 1))\n",
    "    image = image.astype('float32') / 255.0\n",
    "    \n",
    "    # Predict the label\n",
    "    prediction = model.predict(image)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    \n",
    "    # Print the predicted label\n",
    "    print(\"Image:\", image_path)\n",
    "    print(\"Predicted Label:\", predicted_label)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d41b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import keras\n",
    "import cv2\n",
    "\n",
    "# Preprocess the image\n",
    "image_path = 'a (1).jpg'\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((32, 32))  # Resize the image to match model input size\n",
    "image = np.array(image)\n",
    "gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = gray_img.astype('float32') / 255.0  # Normalize pixel values\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Load the model\n",
    "model = keras.models.load_model('model.h5')\n",
    "\n",
    "# Perform inference\n",
    "predictions = model.predict(image)\n",
    "\n",
    "# Interpret the results\n",
    "# Example: Get the predicted class label\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Draw bounding box on the actual image\n",
    "image = cv2.imread(image_path)\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "# Define the bounding box coordinates (example: using random values)\n",
    "x_min = 50\n",
    "y_min = 50\n",
    "x_max = 200\n",
    "y_max = 200\n",
    "\n",
    "# Draw the bounding box rectangle on the image\n",
    "cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding box\n",
    "cv2.imshow('Image with Bounding Box', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
